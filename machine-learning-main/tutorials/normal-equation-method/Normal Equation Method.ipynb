{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal equation is a method used in machine learning for finding the optimal parameters of a linear regression model. It provides a closed-form solution that minimizes the cost function and allows us to calculate the optimal values of the model's coefficients without iterative optimization algorithms like gradient descent.\n",
    "\n",
    "The linear regression model can be represented as:\n",
    "\n",
    "scss\n",
    "Copy code\n",
    "h(x) = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₙxₙ\n",
    "where h(x) is the predicted output, θ₀, θ₁, θ₂, ..., θₙ are the coefficients (also known as weights or parameters), and x₁, x₂, ..., xₙ are the input features.\n",
    "\n",
    "The goal is to find the values of θ₀, θ₁, θ₂, ..., θₙ that minimize the difference between the predicted outputs and the actual outputs in the training data. This can be achieved by minimizing the cost function, typically the mean squared error (MSE):\n",
    "\n",
    "scss\n",
    "Copy code\n",
    "J(θ) = (1/2m) ∑[h(xᵢ) - yᵢ]²\n",
    "where m is the number of training examples, h(xᵢ) is the predicted output for the ith example, and yᵢ is the actual output for the ith example.\n",
    "\n",
    "To find the optimal values of the coefficients, we can use the normal equation:\n",
    "\n",
    "scss\n",
    "Copy code\n",
    "θ = (XᵀX)⁻¹Xᵀy\n",
    "where θ is the vector of coefficients, X is the matrix of input features (with each row representing a training example and each column representing a feature), and y is the vector of actual outputs.\n",
    "\n",
    "Here's a step-by-step breakdown of the normal equation method:\n",
    "\n",
    "Add a column of ones to the left of the X matrix to represent the intercept term (i.e., x₀ = 1 for all training examples).\n",
    "\n",
    "Compute the transpose of X, denoted as Xᵀ.\n",
    "\n",
    "Compute the product of Xᵀ and X, denoted as XᵀX.\n",
    "\n",
    "Compute the inverse of XᵀX, denoted as (XᵀX)⁻¹.\n",
    "\n",
    "Compute the product of (XᵀX)⁻¹ and Xᵀ, denoted as (XᵀX)⁻¹Xᵀ.\n",
    "\n",
    "Compute the product of (XᵀX)⁻¹Xᵀ and y, denoted as θ.\n",
    "\n",
    "The resulting θ values will give you the optimal coefficients for the linear regression model. This approach provides an analytical solution to the optimization problem and is particularly useful when the number of features is relatively small. However, it may become computationally expensive and inefficient when dealing with a large number of features, as matrix inversion can be computationally intensive. In such cases, gradient descent or other iterative optimization algorithms are typically preferred."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
